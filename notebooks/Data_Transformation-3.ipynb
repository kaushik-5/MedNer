{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a9cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d42d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8516e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_files_dir = r'E:\\project\\annotated_dictionary'\n",
    "bio_files = [os.path.join(bio_files_dir, f) for f in os.listdir(r'E:\\project\\annotated_dictionary') if f.endswith('.bio')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebfd940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of .bio files is 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of .bio files is {len(bio_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9f940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any of the stopwords contain B-tag\n",
    "for bio_file in bio_files:\n",
    "    with open(bio_file, \"r\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            word, tag = line.strip().split('\\t')\n",
    "            if word in STOP_WORDS and tag.startswith('B'):\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09853edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def clean_word(word):\n",
    "    # remove non-alphanumeric characters and extra whitespaces\n",
    "    word = re.sub(r'[^\\w\\s]','',word)\n",
    "    word = re.sub(r'\\s+',' ',word)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    word = word.lower()\n",
    "\n",
    "    # lemmatize the word\n",
    "    lemma = nlp(word)[0].lemma_\n",
    "    \n",
    "    # check if the lemma is a stop word\n",
    "    if lemma not in STOP_WORDS:\n",
    "        return lemma\n",
    "    \n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141f77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(bio_file):\n",
    "    \"\"\"\n",
    "      Reads the input file, which is assumed to have one token per line with tab-separated word and tag, \n",
    "      and extracts the sentences and corresponding BIO tags. \n",
    "      The returned tuple allows for easy access to both the words and their corresponding tags for further processing.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []    \n",
    "    with open(bio_file, \"r\", encoding='utf-8') as f:\n",
    "        current_sentences = []\n",
    "        current_labels = []\n",
    "        for line in f:\n",
    "            if line.strip() == '':                  \n",
    "                if len(current_sentences) > 0:                 \n",
    "                    sentences.append(current_sentences)\n",
    "                    labels.append(current_labels)\n",
    "                    # Reset the current sentence and labels lists\n",
    "                    current_sentences = []\n",
    "                    current_labels = []\n",
    "                    continue          \n",
    "            word, tag = line.strip().split('\\t')\n",
    "            word = clean_word(word) \n",
    "            if word.strip():\n",
    "                current_sentences.append(word)\n",
    "                if len(current_labels) > 0:\n",
    "                    if tag[2:] == current_labels[-1][2:] and tag[:2] == \"B-\":\n",
    "                        tag = f\"I-{tag[2:]}\"\n",
    "                current_labels.append(tag)      \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd844f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bio_files(bio_files): \n",
    "    sentences = []\n",
    "    labels = []    \n",
    "    for idx, bio_file in enumerate(bio_files):        \n",
    "        curr_sentences, curr_labels = parse_data_from_file(bio_file)        \n",
    "        if len(curr_sentences) > 0:\n",
    "            sentences.extend(curr_sentences)\n",
    "            labels.extend(curr_labels)            \n",
    "        if (idx+1) % 20 == 0:\n",
    "            print(f'{idx+1} completed')\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45fd1f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 completed\n",
      "40 completed\n",
      "60 completed\n",
      "80 completed\n",
      "100 completed\n",
      "120 completed\n",
      "140 completed\n",
      "160 completed\n",
      "180 completed\n",
      "200 completed\n"
     ]
    }
   ],
   "source": [
    "sentences,labels = parse_bio_files(bio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53eabb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 4341 examples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset contains {len(sentences)} examples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30bb6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "num_train = int(num_sentences * (1 - TEST_SIZE - 0.1))\n",
    "num_valid = int(num_sentences * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0efda6ce-3858-4c0a-b6e1-2bb046a5a6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3038"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1d5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = sentences[:num_train]\n",
    "train_labels = labels[:num_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2c1a53a-a4a5-4027-a235-076d5b6a48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentences = sentences[num_train:num_train+num_valid]\n",
    "valid_labels = labels[num_train:num_train+num_valid]\n",
    "\n",
    "test_sentences = sentences[num_train+num_valid:]\n",
    "test_labels = labels[num_train+num_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de6ea2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(element for sublist in labels for element in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e0dbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "index_to_label = {id: label for label, id in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ba093e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding padding to the dictionary\n",
    "label_to_index['<PAD>'] = 79\n",
    "index_to_label[79] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4f7383a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(index_to_label)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "897d33db-562b-4ead-985c-0e00a072a793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ACT': 1, 'B-ADM': 2, 'B-AGE': 3, 'B-ARA': 4, 'B-BAT': 5, 'B-BST': 6, 'B-CLE': 7, 'B-COL': 8, 'B-COR': 9, 'B-DAT': 10, 'B-DET': 11, 'B-DIA': 12, 'B-DIS': 13, 'B-DOS': 14, 'B-DUR': 15, 'B-FAM': 16, 'B-FRE': 17, 'B-HEI': 18, 'B-HIS': 19, 'B-LAB': 20, 'B-MAS': 21, 'B-MED': 22, 'B-NBL': 23, 'B-OCC': 24, 'B-OTE': 25, 'B-OTH': 26, 'B-OUT': 27, 'B-PER': 28, 'B-QUC': 29, 'B-SEV': 30, 'B-SEX': 31, 'B-SHA': 32, 'B-SIG': 33, 'B-SUB': 34, 'B-TEX': 35, 'B-THP': 36, 'B-TIM': 37, 'B-VOL': 38, 'B-WEI': 39, 'I-ACT': 40, 'I-ADM': 41, 'I-AGE': 42, 'I-ARA': 43, 'I-BAT': 44, 'I-BST': 45, 'I-CLE': 46, 'I-COL': 47, 'I-COR': 48, 'I-DAT': 49, 'I-DET': 50, 'I-DIA': 51, 'I-DIS': 52, 'I-DOS': 53, 'I-DUR': 54, 'I-FAM': 55, 'I-FRE': 56, 'I-HEI': 57, 'I-HIS': 58, 'I-LAB': 59, 'I-MAS': 60, 'I-MED': 61, 'I-NBL': 62, 'I-OCC': 63, 'I-OTE': 64, 'I-OTH': 65, 'I-OUT': 66, 'I-PER': 67, 'I-QUC': 68, 'I-SEV': 69, 'I-SHA': 70, 'I-SIG': 71, 'I-SUB': 72, 'I-TEX': 73, 'I-THP': 74, 'I-TIM': 75, 'I-VOL': 76, 'I-WEI': 77, 'O': 78, '<PAD>': 79}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91f9e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "MAX_LENGTH = 100\n",
    "#transforming the labels to numerical representation\n",
    "train_labels = [[label_to_index[label] for label in labels] for labels in train_labels]\n",
    "train_labels = pad_sequences(train_labels, maxlen=MAX_LENGTH, padding='post', value=NUM_CLASSES-1)\n",
    "train_labels = to_categorical(train_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "valid_labels = [[label_to_index[label] for label in labels] for labels in valid_labels]\n",
    "valid_labels = pad_sequences(valid_labels, maxlen=MAX_LENGTH, padding='post', value=NUM_CLASSES-1)\n",
    "valid_labels = to_categorical(valid_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "test_labels = [[label_to_index[label] for label in labels] for labels in test_labels]\n",
    "test_labels = pad_sequences(test_labels, maxlen=MAX_LENGTH, padding='post', value=NUM_CLASSES-1)\n",
    "test_labels = to_categorical(test_labels, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d73a137-843d-48e6-9d02-8ee9e4736faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0],valid_labels[0],test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5fc9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input sentences to sequences of word indices\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff8e923c-1fd3-4daa-ab63-422af1c584a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 2726,   32,  448, 2018, 1420,    5,   81,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdb497",
   "metadata": {},
   "source": [
    "###  save to a .npz file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27d0e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    r'E:\\project\\final_npz',\n",
    "     train_sequences_padded=train_sequences_padded,\n",
    "     train_labels=train_labels,\n",
    "     val_sequences_padded=val_sequences_padded,\n",
    "     val_labels=valid_labels,\n",
    "     test_sequences_padded=test_sequences_padded,\n",
    "     test_labels=test_labels,\n",
    "     label_to_index=label_to_index,\n",
    "     index_to_label=index_to_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "886f4882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3038, 100, 79)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cd0ea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays in the .npz file: ['train_sequences_padded', 'train_labels', 'val_sequences_padded', 'val_labels', 'test_sequences_padded', 'test_labels', 'label_to_index', 'index_to_label']\n",
      "Array 'train_sequences_padded':\n",
      "[[ 301  522    5 ...    0    0    0]\n",
      " [ 102  614  910 ...    0    0    0]\n",
      " [ 673  285  523 ...    0    0    0]\n",
      " ...\n",
      " [ 122  177    2 ...    0    0    0]\n",
      " [  91  179 1566 ...    0    0    0]\n",
      " [ 156 1948 2528 ...    0    0    0]]\n",
      "Array 'train_labels':\n",
      "[[[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "Array 'val_sequences_padded':\n",
      "[[ 281   18 5019 ...    0    0    0]\n",
      " [   9   17  307 ...    0    0    0]\n",
      " [   9   58  267 ...    0    0    0]\n",
      " ...\n",
      " [1900  102    0 ...    0    0    0]\n",
      " [  53   37    2 ...    0    0    0]\n",
      " [   1 1042   99 ...    0    0    0]]\n",
      "Array 'val_labels':\n",
      "[[[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "Array 'test_sequences_padded':\n",
      "[[   1 2726   32 ...    0    0    0]\n",
      " [ 690   60  218 ...    0    0    0]\n",
      " [1042   99 1505 ...    0    0    0]\n",
      " ...\n",
      " [1319  100  678 ...    0    0    0]\n",
      " [ 272    9  699 ...    0    0    0]\n",
      " [  24   10   49 ...    0    0    0]]\n",
      "Array 'test_labels':\n",
      "[[[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n",
      "Array 'label_to_index':\n",
      "{'B-ACT': 1, 'B-ADM': 2, 'B-AGE': 3, 'B-ARA': 4, 'B-BAT': 5, 'B-BST': 6, 'B-CLE': 7, 'B-COL': 8, 'B-COR': 9, 'B-DAT': 10, 'B-DET': 11, 'B-DIA': 12, 'B-DIS': 13, 'B-DOS': 14, 'B-DUR': 15, 'B-FAM': 16, 'B-FRE': 17, 'B-HEI': 18, 'B-HIS': 19, 'B-LAB': 20, 'B-MAS': 21, 'B-MED': 22, 'B-NBL': 23, 'B-OCC': 24, 'B-OTE': 25, 'B-OTH': 26, 'B-OUT': 27, 'B-PER': 28, 'B-QUC': 29, 'B-SEV': 30, 'B-SEX': 31, 'B-SHA': 32, 'B-SIG': 33, 'B-SUB': 34, 'B-TEX': 35, 'B-THP': 36, 'B-TIM': 37, 'B-VOL': 38, 'B-WEI': 39, 'I-ACT': 40, 'I-ADM': 41, 'I-AGE': 42, 'I-ARA': 43, 'I-BAT': 44, 'I-BST': 45, 'I-CLE': 46, 'I-COL': 47, 'I-COR': 48, 'I-DAT': 49, 'I-DET': 50, 'I-DIA': 51, 'I-DIS': 52, 'I-DOS': 53, 'I-DUR': 54, 'I-FAM': 55, 'I-FRE': 56, 'I-HEI': 57, 'I-HIS': 58, 'I-LAB': 59, 'I-MAS': 60, 'I-MED': 61, 'I-NBL': 62, 'I-OCC': 63, 'I-OTE': 64, 'I-OTH': 65, 'I-OUT': 66, 'I-PER': 67, 'I-QUC': 68, 'I-SEV': 69, 'I-SHA': 70, 'I-SIG': 71, 'I-SUB': 72, 'I-TEX': 73, 'I-THP': 74, 'I-TIM': 75, 'I-VOL': 76, 'I-WEI': 77, 'O': 78, '<PAD>': 79}\n",
      "Array 'index_to_label':\n",
      "{1: 'B-ACT', 2: 'B-ADM', 3: 'B-AGE', 4: 'B-ARA', 5: 'B-BAT', 6: 'B-BST', 7: 'B-CLE', 8: 'B-COL', 9: 'B-COR', 10: 'B-DAT', 11: 'B-DET', 12: 'B-DIA', 13: 'B-DIS', 14: 'B-DOS', 15: 'B-DUR', 16: 'B-FAM', 17: 'B-FRE', 18: 'B-HEI', 19: 'B-HIS', 20: 'B-LAB', 21: 'B-MAS', 22: 'B-MED', 23: 'B-NBL', 24: 'B-OCC', 25: 'B-OTE', 26: 'B-OTH', 27: 'B-OUT', 28: 'B-PER', 29: 'B-QUC', 30: 'B-SEV', 31: 'B-SEX', 32: 'B-SHA', 33: 'B-SIG', 34: 'B-SUB', 35: 'B-TEX', 36: 'B-THP', 37: 'B-TIM', 38: 'B-VOL', 39: 'B-WEI', 40: 'I-ACT', 41: 'I-ADM', 42: 'I-AGE', 43: 'I-ARA', 44: 'I-BAT', 45: 'I-BST', 46: 'I-CLE', 47: 'I-COL', 48: 'I-COR', 49: 'I-DAT', 50: 'I-DET', 51: 'I-DIA', 52: 'I-DIS', 53: 'I-DOS', 54: 'I-DUR', 55: 'I-FAM', 56: 'I-FRE', 57: 'I-HEI', 58: 'I-HIS', 59: 'I-LAB', 60: 'I-MAS', 61: 'I-MED', 62: 'I-NBL', 63: 'I-OCC', 64: 'I-OTE', 65: 'I-OTH', 66: 'I-OUT', 67: 'I-PER', 68: 'I-QUC', 69: 'I-SEV', 70: 'I-SHA', 71: 'I-SIG', 72: 'I-SUB', 73: 'I-TEX', 74: 'I-THP', 75: 'I-TIM', 76: 'I-VOL', 77: 'I-WEI', 78: 'O', 79: '<PAD>'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(r'E:\\project\\final_npz.npz',allow_pickle=True)\n",
    "\n",
    "# List the arrays stored in the .npz file\n",
    "array_names = data.files\n",
    "print(\"Arrays in the .npz file:\", array_names)\n",
    "\n",
    "# Access and view the contents of individual arrays\n",
    "for array_name in array_names:\n",
    "    array_data = data[array_name]\n",
    "    print(f\"Array '{array_name}':\")\n",
    "    print(array_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "735fd202-190c-4c39-b94a-8669917a4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "The shape of input ids tensor of train data is (2696, 200)\n",
      "The shape of attention masks tensor of train data is (2696, 200)\n",
      "The shape of labels tensor of train data is (2696, 200)\n",
      "\n",
      "VALIDATION DATA\n",
      "The shape of input ids tensor of validation data is (955, 200)\n",
      "The shape of attention masks tensor of validation data is (955, 200)\n",
      "The shape of labels tensor of validation data is (955, 200)\n",
      "\n",
      "TEST DATA\n",
      "The shape of input ids tensor of test data is (929, 200)\n",
      "The shape of attention masks tensor of test data is (929, 200)\n",
      "The shape of labels tensor of test data is (929, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING DATA\")\n",
    "print(f\"The shape of input ids tensor of train data is {train_data['input_ids'].shape}\")\n",
    "print(f\"The shape of attention masks tensor of train data is {train_data['attention_mask'].shape}\")\n",
    "print(f\"The shape of labels tensor of train data is {train_data['labels'].shape}\")\n",
    "\n",
    "print(\"\\nVALIDATION DATA\")\n",
    "print(f\"The shape of input ids tensor of validation data is {val_data['input_ids'].shape}\")\n",
    "print(f\"The shape of attention masks tensor of validation data is {val_data['attention_mask'].shape}\")\n",
    "print(f\"The shape of labels tensor of validation data is {val_data['labels'].shape}\")\n",
    "\n",
    "print(\"\\nTEST DATA\")\n",
    "print(f\"The shape of input ids tensor of test data is {test_data['input_ids'].shape}\")\n",
    "print(f\"The shape of attention masks tensor of test data is {test_data['attention_mask'].shape}\")\n",
    "print(f\"The shape of labels tensor of test data is {test_data['labels'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
